# Set seed for reproducibility
set.seed(606)

# Parameters
n_respondents <- 200  # Number of respondents
n_items <- 100       # Number of items
options <- c("A", "B", "C", "D")  # Response options
n_factors <- 3       # Number of underlying dimensions

# Assign items to three dimensions (roughly equal split)
items_per_factor <- c(34, 33, 33)  # 34 items for Factor 1, 33 for Factor 2, 33 for Factor 3
factor_assignments <- c(rep("Factor1", items_per_factor[1]), 
                        rep("Factor2", items_per_factor[2]), 
                        rep("Factor3", items_per_factor[3]))
item_names <- paste0("Item", 1:n_items)

# Simulate latent factor scores for each respondent (theta)
theta <- matrix(rnorm(n_respondents * n_factors, mean = 0, sd = 1), 
                nrow = n_respondents, ncol = n_factors)
colnames(theta) <- paste0("Factor", 1:n_factors)

# Define factor loadings (strong loadings on assigned factor, weak cross-loadings)
loadings <- matrix(0, nrow = n_items, ncol = n_factors)
for (i in 1:n_items) {
  if (factor_assignments[i] == "Factor1") {
    loadings[i, ] <- c(0.8, rnorm(1, 0, 0.1), rnorm(1, 0, 0.1))  # Strong loading on Factor 1
  } else if (factor_assignments[i] == "Factor2") {
    loadings[i, ] <- c(rnorm(1, 0, 0.1), 0.8, rnorm(1, 0, 0.1))  # Strong loading on Factor 2
  } else {
    loadings[i, ] <- c(rnorm(1, 0, 0.1), rnorm(1, 0, 0.1), 0.8)  # Strong loading on Factor 3
  }
}

# Generate latent item scores
latent_scores <- theta %*% t(loadings) + rnorm(n_respondents * n_items, mean = 0, sd = 0.5)

# Convert latent scores to ordinal responses (A, B, C, D)
dataset <- matrix(NA, nrow = n_respondents, ncol = n_items)
for (i in 1:n_items) {
  # Map latent scores to probabilities using thresholds
  thresholds <- quantile(latent_scores[, i], probs = c(0.25, 0.50, 0.75))
  for (j in 1:n_respondents) {
    score <- latent_scores[j, i]
    if (score <= thresholds[1]) {
      dataset[j, i] <- "A"
    } else if (score <= thresholds[2]) {
      dataset[j, i] <- "B"
    } else if (score <= thresholds[3]) {
      dataset[j, i] <- "C"
    } else {
      dataset[j, i] <- "D"
    }
  }
}

# Convert to data frame
dataset <- as.data.frame(dataset)
colnames(dataset) <- item_names

# Create item info data frame
item_info <- data.frame(
  Item = item_names,
  Factor = factor_assignments
)

# Save dataset and item info to CSV
write.csv(dataset, "factor_analysis_dataset.csv", row.names = FALSE)
write.csv(item_info, "factor_analysis_item_info.csv", row.names = FALSE)

# Display first few rows
head(dataset[, 1:10])  # Show first 10 items for brevity

# Display summary of responses
summary(dataset)

# Convert responses to numeric for factor analysis (A=1, B=2, C=3, D=4)
numeric_dataset <- as.data.frame(lapply(dataset, function(x) as.numeric(factor(x, levels = c("A", "B", "C", "D")))))

# Perform exploratory factor analysis (requires psych package)
if (!require(psych)) install.packages("psych")
library(psych)

# Run EFA with 3 factors
efa_result <- fa(numeric_dataset, nfactors = 3, fm = "ml", rotate = "varimax")

# Display factor loadings
print("Factor Loadings:")
print(efa_result$loadings, cutoff = 0.3)

# Display communalities and variance explained
print("Communalities:")
summary(efa_result$communality)
print("Variance Explained by Factors:")
efa_result$Vaccounted

# Calculate Cronbach's alpha for each factor
factor1_items <- numeric_dataset[, 1:34]
factor2_items <- numeric_dataset[, 35:67]
factor3_items <- numeric_dataset[, 68:100]
alpha_f1 <- alpha(factor1_items)$total$std.alpha
alpha_f2 <- alpha(factor2_items)$total$std.alpha
alpha_f3 <- alpha(factor3_items)$total$std.alpha

print("Cronbach's Alpha by Factor:")
cat("Factor 1:", alpha_f1, "\n")
cat("Factor 2:", alpha_f2, "\n")
cat("Factor 3:", alpha_f3, "\n")
